# ML Experiments
This repo packages up code for reproducible ML experiments, including:
- Hyperparameter tuning (using grid search, random search, tpe, or atpe) (with optional cross validation)
- Model evaluation (PR curve, PR vs threshold, ROC curve, score distribution vs label, KS-statistic, various tables, etc.)
- Model explainability objects (shapely values, PSI, CSI, VIF, WoE/IV, permutation feature importance, correlation matrix)
- XGBoost-related objects (plots of n_estimators vs performance metrics, tree-based feature importance)
- Many other features
